# Ollama LLM configuration
OLLAMA_LLM_HOST=https://gpu.aet.cit.tum.de/ollama
OLLAMA_LLM_KEY=your-ollama-api-key

# Google GenAI configuration
# GOOGLE_API_KEY=your_api_key

# OpenAI configuration
# OPENAI_API_KEY=your_openai_api_key

# AWS Bedrock configuration
# AWS_REGION=eu-north-1
# AWS_ACCESS_KEY_ID="your_aws_access_key_id"
# AWS_SECRET_ACCESS_KEY="your_aws_secret_access_key"
# AWS_SESSION_TOKEN="your_aws_session_token"

# LLM models to use for different tasks
# These models must be available in your Ollama instance
SPLITTING_MODEL=deepseek-r1:70b
SLIDESGEN_MODEL=deepseek-r1:8b
